{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter # getting max q from a dict\n",
    "import random\n",
    "import numpy\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "class mdp():\n",
    "    def __init__(self, error_prob):\n",
    "\n",
    "        self.actions = [\"top\",\n",
    "                        \"left\",\n",
    "                        \"right\",\n",
    "                        \"bottom\"]\n",
    "\n",
    "        self.q = {}\n",
    "\n",
    "        self.state = {}\n",
    "        \n",
    "        # RL parameters.\n",
    "        self.alpha = 0.1 # learning rate\n",
    "        self.epsilon = 0.5 # explore vs exploit\n",
    "        self.gamma = 0.9 # future reward discount \n",
    "        self.error_prob = error_prob\n",
    "\n",
    "        # Initialise the environment.\n",
    "        self.reset()\n",
    "        \n",
    "    def set_error_prob(self, error_prob):\n",
    "        self.error_prob = error_prob\n",
    "        \n",
    "    def reset(self):\n",
    "\n",
    "        self.state = {\n",
    "            \"task\": \"start\",\n",
    "            \"position\": \"s1\"}\n",
    "\n",
    "        # Start with no previous state, no current state, no previous\n",
    "        # action, and no current action.\n",
    "        self.previous_state = None\n",
    "        self.current_state = None\n",
    "        self.previous_action = None\n",
    "        self.action = None\n",
    "        \n",
    "    def update_environment(self):\n",
    "\n",
    "        position = self.state[\"position\"]\n",
    "        if position == \"s2\":\n",
    "            # with a prob entering the error state\n",
    "            if random.random() < self.error_prob:\n",
    "                self.state[\"position\"] = \"e1\"\n",
    "            else:\n",
    "                if self.action == \"left\": \n",
    "                    self.state[\"position\"] = \"s1\"\n",
    "                elif self.action == \"right\":\n",
    "                    self.state[\"position\"] = \"s3\"\n",
    "        else:\n",
    "            if self.action == \"left\":        \n",
    "                if position == \"s3\":\n",
    "                    self.state[\"position\"] = \"s2\"\n",
    "                if position == \"s4\":\n",
    "                    self.state[\"position\"] = \"s3\"\n",
    "                if position == \"e3\":\n",
    "                    self.state[\"position\"] = \"e2\"\n",
    "                if position == \"e4\":\n",
    "                    self.state[\"position\"] = \"e3\"\n",
    "            if self.action == \"right\":\n",
    "                if position == \"s1\":\n",
    "                    self.state[\"position\"] = \"s2\"\n",
    "                if position == \"s3\":\n",
    "                    self.state[\"position\"] = \"s4\"\n",
    "                if position == \"e2\":\n",
    "                    self.state[\"position\"] = \"e3\"\n",
    "                if position == \"e3\":\n",
    "                    self.state[\"position\"] = \"e4\"\n",
    "                if position == \"s4\":\n",
    "                    self.state[\"task\"] = \"done\"\n",
    "            if self.action == \"top\":\n",
    "                if position == \"e2\":\n",
    "                    self.state[\"position\"] = \"e1\"\n",
    "                if position == \"e3\":\n",
    "                    self.state[\"position\"] = \"e6\"\n",
    "                if position == \"e4\":\n",
    "                    self.state[\"position\"] = \"e5\"\n",
    "                if position == \"e6\":\n",
    "                    self.state[\"position\"] = \"s3\"\n",
    "            if self.action == \"bottom\":\n",
    "                if position == \"e1\":\n",
    "                    self.state[\"position\"] = \"e2\"\n",
    "                if position == \"e6\":\n",
    "                    self.state[\"position\"] = \"e3\"\n",
    "                if position == \"e5\":\n",
    "                    self.state[\"position\"] = \"e4\"\n",
    "\n",
    "    def calculate_reward(self):\n",
    "        self.reward = 0\n",
    "        position = self.state[\"position\"]\n",
    "        action = self.action\n",
    "#         if action == \"left\" and (position == \"s1\" or position == \"s2\" or position == \"s3\" or position == \"e2\" or position == \"e3\"):\n",
    "#             self.reward += -2\n",
    "#         if action == \"right\" and (position == \"s2\" or position == \"s3\" or position == \"e3\"):\n",
    "#             self.reward += -1\n",
    "#         if action == \"bottom\" and (position == \"e1\"):\n",
    "#             self.reward += -3\n",
    "#         if action == \"bottom\" and (position == \"e2\" or position == \"e4\"):\n",
    "#             self.reward += -2\n",
    "        self.reward += -1\n",
    "        if self.state[\"task\"] == \"done\":\n",
    "            self.reward += 20\n",
    "            \n",
    "    def choose_action_epsilon_greedy(self):\n",
    "        if random.random() < self.epsilon:\n",
    "            self.action = random.choice(self.actions)\n",
    "            return \"randomly\" # for output (debug) purposes\n",
    "        else:\n",
    "            self.action = max(self.q[self.current_state].items(), key = itemgetter(1))[0]\n",
    "            return \"greedily\"\n",
    "        \n",
    "    def print_state(self):\n",
    "        print(\"Current state:\")\n",
    "        for s in self.state:\n",
    "            print(\"   \", s, \":\", self.state[s])\n",
    "            print()\n",
    "            \n",
    "    def print_q(self, state = None):\n",
    "        for s in self.q:\n",
    "            print(s)\n",
    "            if state == None or repr(state) == s:\n",
    "                for a in self.actions:\n",
    "                    print(\"   \", a, \":\", round(self.q[s][a],2))\n",
    "                    \n",
    "    def update_q_learning(self):\n",
    "        # Only learn if there is a previous action. If this is a start\n",
    "        # of a new episode after a self.reset(), cannot learn yet.\n",
    "        if self.previous_action != None:\n",
    "            previous_q = self.q[self.previous_state][self.previous_action]\n",
    "            next_q = max(self.q[self.current_state].items(), key = itemgetter(1))[1]\n",
    "            self.q[self.previous_state][self.previous_action] = \\\n",
    "                previous_q + self.alpha * (self.reward + self.gamma * next_q - previous_q)\n",
    "            \n",
    "    def update_q_td(self):\n",
    "        previous_q = self.q[self.current_state][self.action]\n",
    "        self.q[self.current_state][self.action] = \\\n",
    "                previous_q + self.alpha * (self.reward - previous_q)\n",
    "    \n",
    "    def iterate_model(self, print_progress = False, force_action = None, Testing = False):\n",
    "        self.previous_state = self.current_state\n",
    "        self.previous_action = self.action\n",
    "\n",
    "        self.current_state = repr(self.state)\n",
    "       \n",
    "        if self.current_state not in self.q:\n",
    "            self.q[self.current_state] = {}\n",
    "            # Add all actions as possible pairs if this new state.\n",
    "            for a in self.actions:\n",
    "                self.q[self.current_state][a] = 0.0\n",
    "\n",
    "        self.update_q_learning()\n",
    "\n",
    "        # Choose action, store the explore or exploit as a string for\n",
    "        # outputting (debug). If the action is forced, take that action.\n",
    "        if force_action:\n",
    "            self.action = force_action\n",
    "            how = \"forced\"\n",
    "        else:\n",
    "            how = self.choose_action_epsilon_greedy()\n",
    "\n",
    "        if print_progress: print(\"Took action <\", self.action, \"> \", how, sep = '')\n",
    "        # Based on the action and the current state, update the environment.\n",
    "        self.update_environment()\n",
    "\n",
    "        self.calculate_reward()\n",
    "        if print_progress: print(\"Reward =\", self.reward)\n",
    "\n",
    "        # If the state is end state, update using TD learning, because\n",
    "        # Q learning only happens when there is a previous state,\n",
    "        # which reset removes.\n",
    "        if self.state[\"task\"] == \"done\":\n",
    "            if print_progress:\n",
    "                print(\"Task done!\")\n",
    "                self.print_state()\n",
    "            self.update_q_td()\n",
    "            # Reset the environment. The Q-table is retained as\n",
    "            # memory, but the task starts again.\n",
    "            if not Testing:\n",
    "                self.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the model...\n",
      "Q-values of novice agent with error_prob=0\n",
      "{'task': 'start', 'position': 's1'}\n",
      "    top : 9.03\n",
      "    left : 9.03\n",
      "    right : 11.14\n",
      "    bottom : 9.03\n",
      "{'task': 'start', 'position': 's2'}\n",
      "    top : 11.14\n",
      "    left : 9.03\n",
      "    right : 13.49\n",
      "    bottom : 11.14\n",
      "{'task': 'start', 'position': 's3'}\n",
      "    top : 13.49\n",
      "    left : 11.14\n",
      "    right : 16.1\n",
      "    bottom : 13.49\n",
      "{'task': 'start', 'position': 's4'}\n",
      "    top : 16.1\n",
      "    left : 13.49\n",
      "    right : 19.0\n",
      "    bottom : 16.1\n"
     ]
    }
   ],
   "source": [
    "novice_agent = mdp(error_prob=0)\n",
    "print(\"Training the model...\")\n",
    "for i in range(0, 100000):\n",
    "    novice_agent.iterate_model()\n",
    "print(f'Q-values of novice agent with error_prob=0')\n",
    "# Print the learned Q-values?\n",
    "novice_agent.print_q()\n",
    "# print('#######################')\n",
    "# novice_agent.set_error_prob(1)\n",
    "# for i in range(0, 1000):\n",
    "#     novice_agent.iterate_model()\n",
    "# print(f'2nd stage of Q-values')\n",
    "# # Print the learned Q-values?\n",
    "# agent.print_q()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the model...\n",
      "Q-values of expert agent with error_prob=0.5\n",
      "{'task': 'start', 'position': 's1'}\n",
      "    top : -0.19\n",
      "    left : -0.1\n",
      "    right : -0.1\n",
      "    bottom : 0.0\n",
      "{'task': 'start', 'position': 's2'}\n",
      "    top : -0.1\n",
      "    left : 0.0\n",
      "    right : 0.0\n",
      "    bottom : 0.0\n",
      "{'task': 'start', 'position': 'e1'}\n",
      "    top : -0.1\n",
      "    left : -0.1\n",
      "    right : -0.1\n",
      "    bottom : -0.1\n",
      "{'task': 'start', 'position': 'e2'}\n",
      "    top : 0.0\n",
      "    left : 0.0\n",
      "    right : 0.0\n",
      "    bottom : 0.0\n"
     ]
    }
   ],
   "source": [
    "expert_agent = mdp(error_prob=1)\n",
    "print(\"Training the model...\")\n",
    "for i in range(0, 10):\n",
    "    expert_agent.iterate_model()\n",
    "print(f'Q-values of expert agent with error_prob=0.5')\n",
    "# Print the learned Q-values?\n",
    "expert_agent.print_q()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Took action <right> greedily\n",
      "Reward = -1\n",
      "Took action <bottom> greedily\n",
      "Reward = -1\n",
      "Took action <bottom> greedily\n",
      "Reward = -1\n",
      "Took action <right> greedily\n",
      "Reward = -1\n",
      "Took action <top> greedily\n",
      "Reward = -1\n",
      "Took action <top> greedily\n",
      "Reward = -1\n",
      "Took action <right> greedily\n",
      "Reward = -1\n",
      "Took action <right> greedily\n",
      "Reward = 19\n",
      "Task done!\n",
      "Current state:\n",
      "    task : done\n",
      "\n",
      "    position : s4\n",
      "\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "# Test novice\n",
    "novice_agent.set_error_prob(1)\n",
    "novice_agent.reset()\n",
    "novice_agent.epsilon = 0\n",
    "done = False\n",
    "n_trials = 0\n",
    "\n",
    "def terminate(state):\n",
    "    if state[\"task\"] == \"done\":\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "while done is False:\n",
    "#     novice_agent.print_state()\n",
    "#     novice_agent.print_q(novice_agent.state)\n",
    "    novice_agent.iterate_model(print_progress = True, Testing=True)\n",
    "    \n",
    "    n_trials += 1\n",
    "    done = terminate(novice_agent.state)\n",
    "    \n",
    "    if n_trials > 1000:\n",
    "        done = True\n",
    "        \n",
    "print(n_trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expert_agent = mdp(error_prob=0.5)\n",
    "print(\"Training the model...\")\n",
    "for i in range(0, 10000):\n",
    "    expert_agent.iterate_model()\n",
    "print(f'Q-values of expert agent with error_prob=0.5')\n",
    "# Print the learned Q-values?\n",
    "expert_agent.print_q()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current state:\n",
      "    task : start\n",
      "\n",
      "    position : s1\n",
      "\n",
      "Took action <bottom> greedily\n",
      "Reward = -1\n",
      "Took action <left> greedily\n",
      "Reward = -1\n",
      "Took action <right> greedily\n",
      "Reward = -1\n",
      "Took action <left> greedily\n",
      "Reward = -1\n",
      "Took action <top> greedily\n",
      "Reward = -1\n",
      "Took action <left> greedily\n",
      "Reward = -1\n",
      "Took action <right> greedily\n",
      "Reward = -1\n",
      "Took action <bottom> greedily\n",
      "Reward = -1\n",
      "Took action <top> greedily\n",
      "Reward = -1\n",
      "Took action <bottom> greedily\n",
      "Reward = -1\n",
      "Took action <left> greedily\n",
      "Reward = -1\n",
      "Took action <right> greedily\n",
      "Reward = -1\n",
      "Took action <top> greedily\n",
      "Reward = -1\n",
      "Took action <top> greedily\n",
      "Reward = -1\n",
      "Took action <top> greedily\n",
      "Reward = -1\n",
      "Took action <left> greedily\n",
      "Reward = -1\n",
      "Took action <right> greedily\n",
      "Reward = -1\n",
      "Took action <top> greedily\n",
      "Reward = -1\n",
      "Took action <left> greedily\n",
      "Reward = -1\n",
      "Took action <right> greedily\n",
      "Reward = -1\n",
      "Took action <bottom> greedily\n",
      "Reward = -1\n",
      "Took action <bottom> greedily\n",
      "Reward = -1\n",
      "Took action <left> greedily\n",
      "Reward = -1\n",
      "Took action <right> greedily\n",
      "Reward = -1\n",
      "Took action <left> greedily\n",
      "Reward = -1\n",
      "Took action <bottom> greedily\n",
      "Reward = -1\n",
      "Took action <top> greedily\n",
      "Reward = -1\n",
      "Took action <top> greedily\n",
      "Reward = -1\n",
      "Took action <left> greedily\n",
      "Reward = -1\n",
      "Took action <right> greedily\n",
      "Reward = -1\n",
      "Took action <bottom> greedily\n",
      "Reward = -1\n",
      "Took action <right> greedily\n",
      "Reward = -1\n",
      "Took action <right> greedily\n",
      "Reward = -1\n",
      "Took action <top> greedily\n",
      "Reward = -1\n",
      "Took action <top> greedily\n",
      "Reward = -1\n",
      "Took action <left> greedily\n",
      "Reward = -1\n",
      "Took action <right> greedily\n",
      "Reward = -1\n",
      "Took action <bottom> greedily\n",
      "Reward = -1\n",
      "Took action <left> greedily\n",
      "Reward = -1\n",
      "Took action <bottom> greedily\n",
      "Reward = -1\n",
      "Took action <top> greedily\n",
      "Reward = -1\n",
      "Took action <left> greedily\n",
      "Reward = -1\n",
      "Took action <right> greedily\n",
      "Reward = -1\n",
      "Took action <bottom> greedily\n",
      "Reward = -1\n",
      "Took action <right> greedily\n",
      "Reward = -1\n",
      "Took action <right> greedily\n",
      "Reward = -1\n",
      "Took action <bottom> greedily\n",
      "Reward = -1\n",
      "Took action <top> greedily\n",
      "Reward = -1\n",
      "Took action <top> greedily\n",
      "Reward = -1\n",
      "Took action <left> greedily\n",
      "Reward = -1\n",
      "Took action <right> greedily\n",
      "Reward = -1\n",
      "Took action <bottom> greedily\n",
      "Reward = -1\n",
      "Took action <left> greedily\n",
      "Reward = -1\n",
      "Took action <bottom> greedily\n",
      "Reward = -1\n",
      "Took action <left> greedily\n",
      "Reward = -1\n",
      "Took action <left> greedily\n",
      "Reward = -1\n",
      "Took action <bottom> greedily\n",
      "Reward = -1\n",
      "Took action <top> greedily\n",
      "Reward = -1\n",
      "Took action <top> greedily\n",
      "Reward = -1\n",
      "Took action <left> greedily\n",
      "Reward = -1\n",
      "Took action <right> greedily\n",
      "Reward = -1\n",
      "Took action <bottom> greedily\n",
      "Reward = -1\n",
      "Took action <right> greedily\n",
      "Reward = -1\n",
      "Took action <top> greedily\n",
      "Reward = -1\n",
      "Took action <top> greedily\n",
      "Reward = -1\n",
      "Took action <right> greedily\n",
      "Reward = -1\n",
      "Took action <top> greedily\n",
      "Reward = -1\n",
      "Took action <left> greedily\n",
      "Reward = -1\n",
      "Took action <bottom> greedily\n",
      "Reward = -1\n",
      "Took action <top> greedily\n",
      "Reward = -1\n",
      "Took action <left> greedily\n",
      "Reward = -1\n",
      "Took action <bottom> greedily\n",
      "Reward = -1\n",
      "Took action <top> greedily\n",
      "Reward = -1\n",
      "Took action <left> greedily\n",
      "Reward = -1\n",
      "Took action <right> greedily\n",
      "Reward = -1\n",
      "Took action <bottom> greedily\n",
      "Reward = -1\n",
      "Took action <left> greedily\n",
      "Reward = -1\n",
      "Took action <bottom> greedily\n",
      "Reward = -1\n",
      "Took action <top> greedily\n",
      "Reward = -1\n",
      "Took action <bottom> greedily\n",
      "Reward = -1\n",
      "Took action <right> greedily\n",
      "Reward = -1\n",
      "Took action <right> greedily\n",
      "Reward = -1\n",
      "Took action <right> greedily\n",
      "Reward = -1\n",
      "Took action <bottom> greedily\n",
      "Reward = -1\n",
      "Took action <top> greedily\n",
      "Reward = -1\n",
      "Took action <top> greedily\n",
      "Reward = -1\n",
      "Took action <left> greedily\n",
      "Reward = -1\n",
      "Took action <right> greedily\n",
      "Reward = -1\n",
      "Took action <bottom> greedily\n",
      "Reward = -1\n",
      "Took action <left> greedily\n",
      "Reward = -1\n",
      "Took action <bottom> greedily\n",
      "Reward = -1\n",
      "Took action <left> greedily\n",
      "Reward = -1\n",
      "Took action <left> greedily\n",
      "Reward = -1\n",
      "Took action <bottom> greedily\n",
      "Reward = -1\n",
      "Took action <right> greedily\n",
      "Reward = -1\n",
      "Took action <top> greedily\n",
      "Reward = -1\n",
      "Took action <left> greedily\n",
      "Reward = -1\n",
      "Took action <right> greedily\n",
      "Reward = -1\n",
      "Took action <bottom> greedily\n",
      "Reward = -1\n",
      "Took action <right> greedily\n",
      "Reward = -1\n",
      "Took action <right> greedily\n",
      "Reward = -1\n",
      "Took action <bottom> greedily\n",
      "Reward = -1\n",
      "Took action <top> greedily\n",
      "Reward = -1\n",
      "Took action <top> greedily\n",
      "Reward = -1\n",
      "Took action <left> greedily\n",
      "Reward = -1\n",
      "Took action <right> greedily\n",
      "Reward = -1\n",
      "Took action <bottom> greedily\n",
      "Reward = -1\n",
      "Took action <left> greedily\n",
      "Reward = -1\n",
      "Took action <bottom> greedily\n",
      "Reward = -1\n",
      "Took action <left> greedily\n",
      "Reward = -1\n",
      "Took action <top> greedily\n",
      "Reward = -1\n",
      "Took action <top> greedily\n",
      "Reward = -1\n",
      "Took action <left> greedily\n",
      "Reward = -1\n",
      "Took action <right> greedily\n",
      "Reward = -1\n",
      "Took action <bottom> greedily\n",
      "Reward = -1\n",
      "Took action <left> greedily\n",
      "Reward = -1\n",
      "Took action <bottom> greedily\n",
      "Reward = -1\n",
      "Took action <right> greedily\n",
      "Reward = -1\n",
      "Took action <top> greedily\n",
      "Reward = -1\n",
      "Took action <top> greedily\n",
      "Reward = -1\n",
      "Took action <right> greedily\n",
      "Reward = -1\n",
      "Took action <right> greedily\n",
      "Reward = 19\n",
      "Task done!\n",
      "Current state:\n",
      "    task : done\n",
      "\n",
      "    position : s4\n",
      "\n",
      "122\n"
     ]
    }
   ],
   "source": [
    "# Test expert\n",
    "expert_agent.set_error_prob(1)\n",
    "expert_agent.reset()\n",
    "expert_agent.epsilon = 0\n",
    "done = False\n",
    "n_trials = 0\n",
    "\n",
    "def terminate(state):\n",
    "    if state[\"task\"] == \"done\":\n",
    "        return True\n",
    "    return False\n",
    "expert_agent.print_state()\n",
    "\n",
    "while done is False:\n",
    "#     expert_agent.print_state()\n",
    "#     expert_agent.print_q(expert_agent.state)\n",
    "    expert_agent.iterate_model(print_progress = True, Testing=True)\n",
    "    n_trials += 1\n",
    "    done = terminate(expert_agent.state)\n",
    "    \n",
    "    if n_trials > 1000:\n",
    "        done = True\n",
    "        \n",
    "print(n_trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the model...\n",
      "Q-values of novice agent with error_prob=0\n",
      "{'task': 'start', 'position': 's1'}\n",
      "    top : 9.03\n",
      "    left : 9.03\n",
      "    right : 11.14\n",
      "    bottom : 9.03\n",
      "{'task': 'start', 'position': 's2'}\n",
      "    top : 11.14\n",
      "    left : 9.03\n",
      "    right : 13.49\n",
      "    bottom : 11.14\n",
      "{'task': 'start', 'position': 's3'}\n",
      "    top : 13.49\n",
      "    left : 11.14\n",
      "    right : 16.1\n",
      "    bottom : 13.49\n",
      "{'task': 'start', 'position': 's4'}\n",
      "    top : 16.1\n",
      "    left : 13.49\n",
      "    right : 19.0\n",
      "    bottom : 16.1\n",
      "defaultdict(<class 'list'>, {0.1: [17, 25, 25, 16, 9, 10, 9, 10, 9, 8, 8, 8, 8, 8, 9, 8, 8, 8, 8, 8], 0.5: [17, 18, 9, 8, 9, 8, 8, 10, 9, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8], 1: [17, 16, 8, 10, 12, 14, 11, 11, 11, 12, 10, 9, 9, 9, 8, 8, 8, 8, 8, 8]})\n"
     ]
    }
   ],
   "source": [
    "def terminate(state):\n",
    "    if state[\"task\"] == \"done\":\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def test(agent):\n",
    "    done = False\n",
    "    n_trials = 0\n",
    "\n",
    "    while done is False:\n",
    "        agent.iterate_model(print_progress = False, Testing=True)\n",
    "\n",
    "        n_trials += 1\n",
    "        done = terminate(agent.state)\n",
    "\n",
    "        if n_trials > 1000:\n",
    "            done = True\n",
    "            \n",
    "    return n_trials\n",
    "\n",
    "# Test novice\n",
    "novice_agent = mdp(error_prob=0)\n",
    "print(\"Training the model...\")\n",
    "for i in range(0, 10000):\n",
    "    novice_agent.iterate_model()\n",
    "print(f'Q-values of novice agent with error_prob=0')\n",
    "# Print the learned Q-values?\n",
    "novice_agent.print_q()\n",
    "# Save the q table\n",
    "tem_q_table = copy.deepcopy(novice_agent.q)\n",
    "\n",
    "# novice_agent.set_error_prob(1)\n",
    "# novice_agent.reset()\n",
    "# done = False\n",
    "# n_trials = 0\n",
    "# novice_agent.epsilon = 0\n",
    "\n",
    "# while done is False:\n",
    "# #     novice_agent.print_state()\n",
    "# #     novice_agent.print_q(novice_agent.state)\n",
    "#     novice_agent.iterate_model(print_progress = False, Testing=True)\n",
    "\n",
    "#     n_trials += 1\n",
    "#     done = terminate(novice_agent.state)\n",
    "\n",
    "#     if n_trials > 1000:\n",
    "#         done = True\n",
    "        \n",
    "# print(n_trials)\n",
    "\n",
    "# the number of training episodes\n",
    "n_training = [1]+list(range(50,1000,50))\n",
    "n_error_prob = [0.1, 0.5, 1]\n",
    "n_steps = defaultdict(list)\n",
    "\n",
    "for i in n_training:\n",
    "    for ep in n_error_prob:\n",
    "        # training with epsilon = 0.1 and different error_prob\n",
    "        novice_agent.epsilon = 0.1\n",
    "        novice_agent.q = copy.deepcopy(tem_q_table)\n",
    "        novice_agent.set_error_prob(ep)\n",
    "\n",
    "        for j in range(0, i):\n",
    "            novice_agent.iterate_model()\n",
    "\n",
    "        # test\n",
    "        novice_agent.set_error_prob(1)\n",
    "        novice_agent.epsilon = 0\n",
    "        novice_agent.reset()\n",
    "        n_trials = test(novice_agent)\n",
    "        n_steps[ep].append(n_trials)\n",
    "\n",
    "print(n_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x216 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure(figsize=(10,3))\n",
    "ax1 = fig.add_subplot(131)\n",
    "ax2 = fig.add_subplot(132)\n",
    "ax3 = fig.add_subplot(133)\n",
    "ax1.scatter(n_training, n_steps[0.1])\n",
    "ax2.scatter(n_training, n_steps[0.5])\n",
    "ax3.scatter(n_training, n_steps[1])\n",
    "\n",
    "fig.text(0.5, 0, 'the number of training episodes', ha='center')\n",
    "fig.text(0.06, 0.12, 'the number of training episodes', rotation='vertical')\n",
    "fig.suptitle('How many training times can novice become expert')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1, 21, 51])"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=[1,20,30]\n",
    "numpy.cumsum(a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 5, 10, 15, 20, 25, 30, 35, 40, 45, 50]"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[1]+list(range(5,55,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\"{'task': 'start', 'position': 's1'}\": {'top': 8.150301334080034,\n",
       "  'left': 7.948289993249038,\n",
       "  'right': 10.281898759104017,\n",
       "  'bottom': 8.080559021335377},\n",
       " \"{'task': 'start', 'position': 's2'}\": {'top': 7.961185131764275,\n",
       "  'left': 7.310396301600434,\n",
       "  'right': 11.455306874043059,\n",
       "  'bottom': 7.903033332709773},\n",
       " \"{'task': 'start', 'position': 's3'}\": {'top': 13.55541288537817,\n",
       "  'left': 10.304837710010336,\n",
       "  'right': 16.100100414622258,\n",
       "  'bottom': 13.521124882039832},\n",
       " \"{'task': 'start', 'position': 's4'}\": {'top': 16.188794905130614,\n",
       "  'left': 13.529788510726078,\n",
       "  'right': 19.00001020224953,\n",
       "  'bottom': 16.13796232138917},\n",
       " \"{'task': 'start', 'position': 'e1'}\": {'top': -0.967296310796197,\n",
       "  'left': -1.0318868892870319,\n",
       "  'right': -0.6926803999465068,\n",
       "  'bottom': 3.948645800494972},\n",
       " \"{'task': 'start', 'position': 'e2'}\": {'top': -0.44955319743910005,\n",
       "  'left': -0.003797787000318631,\n",
       "  'right': 7.318026243858537,\n",
       "  'bottom': -0.5778869932},\n",
       " \"{'task': 'start', 'position': 'e3'}\": {'top': 10.51059111950946,\n",
       "  'left': 0.3206369523226391,\n",
       "  'right': -0.2450576344378955,\n",
       "  'bottom': 0.7319574997001225},\n",
       " \"{'task': 'start', 'position': 'e6'}\": {'top': 13.385388815021198,\n",
       "  'left': 0.0,\n",
       "  'right': 1.8913844283638723,\n",
       "  'bottom': 0.7609532489329635},\n",
       " \"{'task': 'done', 'position': 's4'}\": {'top': 10.821123009999999,\n",
       "  'left': 0.0,\n",
       "  'right': 0.0,\n",
       "  'bottom': 0.0},\n",
       " \"{'task': 'start', 'position': 'e4'}\": {'top': -0.29701,\n",
       "  'left': 0.6376721618666507,\n",
       "  'right': -0.1,\n",
       "  'bottom': -0.1},\n",
       " \"{'task': 'start', 'position': 'e5'}\": {'top': -0.385219,\n",
       "  'left': -0.29701,\n",
       "  'right': -0.29701,\n",
       "  'bottom': -0.2881}}"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "copy.deepcopy(novice_agent.q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the model...\n",
      "Q-values of novice agent with error_prob=0\n",
      "{'task': 'start', 'position': 's1'}\n",
      "    top : 9.03\n",
      "    left : 9.03\n",
      "    right : 11.14\n",
      "    bottom : 9.03\n",
      "{'task': 'start', 'position': 's2'}\n",
      "    top : 11.14\n",
      "    left : 9.03\n",
      "    right : 13.49\n",
      "    bottom : 11.14\n",
      "{'task': 'start', 'position': 's3'}\n",
      "    top : 13.49\n",
      "    left : 11.14\n",
      "    right : 16.1\n",
      "    bottom : 13.49\n",
      "{'task': 'start', 'position': 's4'}\n",
      "    top : 16.1\n",
      "    left : 13.49\n",
      "    right : 19.0\n",
      "    bottom : 16.1\n",
      "22\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "novice_agent.set_error_prob(1)\n",
    "novice_agent.reset()\n",
    "done = False\n",
    "n_trials = 0\n",
    "\n",
    "def terminate(state):\n",
    "    if state[\"task\"] == \"done\":\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "while done is False:\n",
    "#     novice_agent.print_state()\n",
    "#     novice_agent.print_q(novice_agent.state)\n",
    "    novice_agent.iterate_model(print_progress = False, Testing=True)\n",
    "    \n",
    "    n_trials += 1\n",
    "    done = terminate(novice_agent.state)\n",
    "    \n",
    "    if n_trials > 1000:\n",
    "        done = True\n",
    "        \n",
    "print(n_trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#######################\n",
      "2nd stage of Q-values\n",
      "{'task': 'start', 'position': 's1'}\n",
      "    top : 9.03\n",
      "    left : 9.03\n",
      "    right : 11.01\n",
      "    bottom : 9.03\n",
      "{'task': 'start', 'position': 's2'}\n",
      "    top : 11.14\n",
      "    left : 8.02\n",
      "    right : 10.71\n",
      "    bottom : 11.14\n",
      "{'task': 'start', 'position': 's3'}\n",
      "    top : 13.49\n",
      "    left : 11.14\n",
      "    right : 16.1\n",
      "    bottom : 13.49\n",
      "{'task': 'start', 'position': 's4'}\n",
      "    top : 16.1\n",
      "    left : 13.49\n",
      "    right : 19.0\n",
      "    bottom : 16.1\n",
      "{'task': 'start', 'position': 'e1'}\n",
      "    top : -0.38\n",
      "    left : -0.39\n",
      "    right : -0.46\n",
      "    bottom : -0.45\n",
      "{'task': 'start', 'position': 'e2'}\n",
      "    top : -0.32\n",
      "    left : -0.3\n",
      "    right : -0.36\n",
      "    bottom : -0.29\n",
      "{'task': 'start', 'position': 'e3'}\n",
      "    top : -0.1\n",
      "    left : -0.22\n",
      "    right : -0.19\n",
      "    bottom : -0.29\n",
      "{'task': 'start', 'position': 'e4'}\n",
      "    top : -0.1\n",
      "    left : -0.1\n",
      "    right : 0.0\n",
      "    bottom : -0.1\n",
      "{'task': 'start', 'position': 'e5'}\n",
      "    top : -0.27\n",
      "    left : -0.1\n",
      "    right : -0.1\n",
      "    bottom : -0.1\n",
      "{'task': 'start', 'position': 'e6'}\n",
      "    top : 1.35\n",
      "    left : 0.0\n",
      "    right : 0.0\n",
      "    bottom : 0.0\n",
      "{'task': 'done', 'position': 's4'}\n",
      "    top : 1.9\n",
      "    left : 0.0\n",
      "    right : 0.0\n",
      "    bottom : 0.0\n",
      "11\n"
     ]
    }
   ],
   "source": [
    "print('#######################')\n",
    "novice_agent.set_error_prob(1)\n",
    "for i in range(0, 20):\n",
    "    novice_agent.iterate_model()\n",
    "    \n",
    "print(f'2nd stage of Q-values')\n",
    "# Print the learned Q-values?\n",
    "novice_agent.print_q()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mykernel",
   "language": "python",
   "name": "mykernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
