{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the McGurk effect\n",
    "\n",
    "### Background\n",
    "From the lecture, I know that a cognitive model can simulate the mental steps that people take when they are performing tasks. The McGurk effect is about how the visual perception will influence the auditory perception. We can simulate this process by building a cognitive model. In this model, we can predict if human can get the right information when they find that what they see don't match what they hear."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement the model\n",
    "I plan to build a model to predict how many trials humans need to get right information with visual influence.\n",
    "I'd like to describe the potential contribution of each of the 4 processes to modeling the model of the MccGurk effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import copy\n",
    "import random\n",
    "import numpy as np\n",
    "import scipy.stats as sp\n",
    "import matplotlib.pyplot as plt\n",
    "import jupyter_module_loader\n",
    "jupyter_module_loader.register()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### State Estimation\n",
    "I assume that people will recognize the visual influence according to previous failed experience. As a result, their sensitivity to the McGurk effect would be decreased. If a person is more sensitive to the McGurk effect, he will be more easily influenced by the visual and make mistake.  \n",
    "\n",
    "To support the model, I use 0-1 to indicate the accuracy of the received word. For hearing, the received word keeps unchanged (1) which is also the correct result, but for vision, the number means the degree to match the sound. For example, 1 means that the mouth movement matches the sound and 0 means it is completely different from the sound."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_prior():\n",
    "    return {'identify_target': False,  # if identify the correct word\n",
    "            'aim_visual': 0,  # the aim deviation of visual people suppose to receive\n",
    "            'sensitivity': 0.85} # the sensitivity to McGurk effect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_state():\n",
    "    return {'audio_target': 1,  # the correct result\n",
    "            'visual_target': 0,  # the matching degree of mouth movement and sound\n",
    "            'sensitivity': 0.85}  # the sensitivity to McGurk effect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def state_estimate( belief, obs ):\n",
    "    # update the belief with the new observation.\n",
    "    random_offset = random.random()\n",
    "    # decrease the sensitivity, avoid less than 0\n",
    "    sensitivity = max(0, belief['sensitivity'] - random_offset)\n",
    "    aim_visual = estimate_target(sensitivity)\n",
    "    return {'sensitivity': sensitivity,\n",
    "            'identify_target': obs['identify_target'],\n",
    "            'aim_visual': aim_visual}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_bounds():\n",
    "    return {'audio_noise': 0.01,\n",
    "            'visual_noise': 0.01,\n",
    "            'perceptual_noise': 0.01}    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observation\n",
    "The Observation function is responsible for gathering information from the state. If we already have information from visual and auditory, we can predict what humans would perceive.  I assume the consequence would be influenced by the sensitivity.   \n",
    "There could also be some perception noise, but I don't consider them now in order to simplify the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def observation(state, bounds):\n",
    "    p_target = perceptual_target(state)\n",
    "    estimate = estimate_target(state['sensitivity'])\n",
    "    obs = {'identify_target': p_target > 0.9,\n",
    "           'sensitivity': state['sensitivity'],\n",
    "           'aim_visual': estimate }\n",
    "\n",
    "    return obs\n",
    "\n",
    "def perceptual_target(state):\n",
    "    sensitivity = state['sensitivity']\n",
    "    audio_target = state['audio_target']\n",
    "    visual_target = state['visual_target']\n",
    "    \n",
    "    return audio_target - sensitivity * (1 - visual_target) # one possible formula to compute the perceptual result\n",
    "\n",
    "def estimate_target(sensitivity):\n",
    "    if sensitivity == 0:\n",
    "        return 1\n",
    "    # avoid bigger than 1\n",
    "    return min(1, 0.9 / sensitivity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'identify_target': False, 'sensitivity': 0.85, 'aim_visual': 1}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "observation( init_state(), init_bounds())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Controller\n",
    "Having made an observation, the agent will have information that it can use to guide the selection of an action. In this model, if people fail to identify the correct word, they need to continue hearing and watching in order to succeed. In this process, I would like to change the sensitivity in the `state` from `belief` in order to join in the computation in observation process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def controller( belief ):\n",
    "    if not belief['identify_target']:\n",
    "        action = {'name': 'hear_watch',\n",
    "                  'sensitivity': belief['sensitivity'],\n",
    "                  'aim_visual': belief['aim_visual']}        \n",
    "    else:\n",
    "        action = {'name': 'stop'}\n",
    "    return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Environment\n",
    "Next, the environment function determines what happens when controller selects an action.  \n",
    "When people hear the pronunciation, what they hear is the correct word. When they watch the mouth movement, they would find it may be different from the pronunciation. In this process, I guess there might be some audio noise and visual noise which could influence the result received by the eyes and ears."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def environment( state, action, bounds ):\n",
    "    if action['name'] == 'hear_watch':\n",
    "        random_audio_noise = np.random.normal(0, bounds['audio_noise'], 1)[0]\n",
    "        state['audio_target'] = min(1, 1 + random_audio_noise)\n",
    "        visual_target = action['aim_visual']\n",
    "        random_visual_noise = np.random.normal(0, bounds['visual_noise'], 1)[0]\n",
    "        state['visual_target'] = max(0, visual_target + random_visual_noise)\n",
    "        state['sensitivity'] = action['sensitivity']\n",
    "    return state "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def agent( belief, state, bounds, stop ):\n",
    "    num_trials = 0\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = controller( belief )\n",
    "        print('action', action)\n",
    "        state = environment( state, action, bounds )\n",
    "        print('state', state)\n",
    "        obs = observation(state, bounds )\n",
    "        print('obs', obs)\n",
    "        belief = state_estimate( belief, obs )\n",
    "        print('belief', belief)\n",
    "        \n",
    "        num_trials += 1\n",
    "        done = stop(state)\n",
    "    return num_trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action {'name': 'hear_watch', 'sensitivity': 0.85, 'aim_visual': 0}\n",
      "state {'audio_target': 1, 'visual_target': 0.009548647568407878, 'sensitivity': 0.85}\n",
      "obs {'identify_target': False, 'sensitivity': 0.85, 'aim_visual': 1}\n",
      "belief {'sensitivity': 0.003916331479663748, 'identify_target': False, 'aim_visual': 1}\n",
      "action {'name': 'hear_watch', 'sensitivity': 0.003916331479663748, 'aim_visual': 1}\n",
      "state {'audio_target': 1, 'visual_target': 1.0081396959977496, 'sensitivity': 0.003916331479663748}\n",
      "obs {'identify_target': True, 'sensitivity': 0.003916331479663748, 'aim_visual': 1}\n",
      "belief {'sensitivity': 0, 'identify_target': True, 'aim_visual': 1}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def terminate(state):\n",
    "    return perceptual_target(state) > 0.9\n",
    "\n",
    "agent(init_prior(), init_state(), init_bounds(), terminate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improvement\n",
    "\n",
    "To get an optimal model of the McGurk effect, in State Estimation process, I suppose there can be better methods to decide the decreasing of the sensitivity or some other factors that may change the visual influence.  \n",
    "\n",
    "In Control process, in addition to hear and watch at the same time, there can be more other actions. For example, people can select actions of closing their eyes so that they can get results only depend on the sound.  \n",
    "\n",
    "In Environment process, the distribution of the noise should be specified. It may be associated with the distance.  \n",
    "\n",
    "In Observation process, how to compute the final perceived result according to hearing and vision is a problem. I think it should be measured in some experiments. And of course, there should be perceptual noise which would influence our brain to receive the result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implications in HCI\n",
    "Since the McGurk effect suggests that our senses make optimal use of visual perception to reduce ambiguity in auditory perception, in some interface, we can combine the vision and the sound together to improve users' experience. For example, some websites enable users to identify the verification codes by hearing and seeing which increase the accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kalman filter for vision dominance\n",
    "From the lecture, we know that the filter takes two estimates (z1 and z2) of a variable z and estimates of the noise in these variables (sigma1 and sigma2). It returns a new estimate z3 and an estimate of the noise sigma3. The new estimate balances the previous estimates and gets better prediction.\n",
    "According to the McGurk effect, we can infer that vision estimation can correct the previous estimation (sound), and then get a new estimation which has less uncertainty than the two previous estimates. When we hear uncertain news, we can utilize our visual perception to reduce ambiguity in auditory perception. If the vision is different from the sound, we can easily be guided to believe what we see and doubt what we hear. In this case, the sound can be easily changed by the vision, so we can see the vision dominate sound."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mykernel",
   "language": "python",
   "name": "mykernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
